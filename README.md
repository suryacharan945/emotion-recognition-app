ğŸ§  Emotion Recognition on Multi-Modal Data using Deep Learning
ğŸ¯ Overview

This project presents a multi-modal emotion recognition system that analyzes image + text data (such as memes) to detect both emotional context and hateful intent.
By combining OpenAIâ€™s CLIP architecture with attention-based fusion, the system captures deeper relationships between visual and textual modalities â€” achieving more accurate and interpretable predictions.

ğŸš€ Features

âœ… Multi-modal hate and emotion detection (Image + Text)
âœ… Uses CLIP for visualâ€“textual feature extraction
âœ… Cross-attention fusion model for deep contextual alignment
âœ… Separate text and image emotion analysis using:

RoBERTa (GoEmotions) for text

ViT-Face-Expression for images
âœ… Streamlit-based interactive UI for easy testing and visualization
âœ… Provides AI-driven interpretive analysis explaining how emotions contribute to the final decision

ğŸ§© Project Structure
ğŸ“ DL-Project/
â”‚
â”œâ”€â”€ app.py                      # Streamlit app script
â”œâ”€â”€ best_hateful_meme_model.pth # Trained multimodal model
â”œâ”€â”€ requirements.txt             # Dependencies for deployment
â””â”€â”€ README.md                    # Project documentation
âš™ï¸ Installation
1ï¸âƒ£ Clone this repository
git clone https://github.com/<your-username>/emotion-recognition-app.git
cd emotion-recognition-app

2ï¸âƒ£ Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate   # for Linux/Mac
venv\Scripts\activate      # for Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the Streamlit app
streamlit run app.py

ğŸ§  Model Architecture
1. Feature Extraction

Image Encoder: CLIP Vision Transformer (openai/clip-vit-base-patch32)

Text Encoder: CLIP Text Transformer
Both produce 512-D embeddings for each modality.

2. Fusion Mechanism

Cross-Attention (torch.nn.MultiheadAttention) aligns visual and textual embeddings.

3. Classification Head

Two-layer fully connected neural network with ReLU + Dropout

Sigmoid output for binary classification (Hateful / Not Hateful)

4. Emotion Analysis

Text Emotion: RoBERTa-GoEmotions

Image Emotion: ViT-Face-Expression

ğŸ§ª Example Use Case

You can upload a meme image and enter its caption.
The system will:

Predict whether the meme is Hateful or Not Hateful.

Extract and display dominant emotions from both image and text.

Generate a human-readable explanation connecting emotions to the final decision.

ğŸ“Š Model Performance
Metric	Value	Comment
Test Accuracy	64.06%	Solid baseline for multimodal hate/emotion classification
ROC-AUC	0.7107	Reliable discrimination ability
Weighted F1	~0.65	Balanced precision and recall
Validation ROC-AUC	0.7381	Good generalization

ğŸ› ï¸ Technologies Used
Category	Tools
Language	Python 3.10
Frameworks	PyTorch, Streamlit
Models	CLIP, RoBERTa-GoEmotions, ViT-Face-Expression
Libraries	Transformers, Pillow, NumPy, Pandas, tqdm
Deployment	Streamlit Cloud / Render
ğŸ’¡ Future Scope

ğŸ”¹ Add audio modality (MFCC + LSTM) for tri-modal emotion fusion
ğŸ”¹ Fine-tune CLIP on emotion-specific datasets for better domain adaptation
ğŸ”¹ Extend to datasets like Flickr8k or AffectNet
ğŸ”¹ Integrate advanced interpretability (Grad-CAM, SHAP) for visual reasoning
